{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Visual_SpatialMultiHeadAttention(nn.Module):\n",
    "    def __init__(self, m: int, H: int) -> None:\n",
    "        super().__init__()\n",
    "        self.m = m\n",
    "        self.H = H\n",
    "        \n",
    "        #making sure m is divisible by  H, otherwise problem!\n",
    "        assert m % H == 0, \"dimensions of model are divisible by number of heads\"\n",
    "        \n",
    "        self.dim_head = m // H\n",
    "        self.w_q = nn.Linear(m, m, bias = False)\n",
    "        self.w_k = nn.Linear(m, m, bias = False)\n",
    "        self.w_v = nn.Linear(m, m, bias = False)\n",
    "        self.w_o = nn.Linear(m, m, bias = False)\n",
    "        \n",
    "    \n",
    "    @staticmethod\n",
    "    def attention(query, key, value):\n",
    "        dim_head = query.shape[-1]\n",
    "        # Just apply the formula from the paper\n",
    "        # (batch, h, seq_len, d_k) --> (batch, h, seq_len, seq_len)\n",
    "        attention_scores = (query @ key.transpose(-2, -1)) / math.sqrt(dim_head)\n",
    "        attention_scores = attention_scores.softmax(dim=-1) # (batch, h, seq_len, seq_len) # Apply softmax\n",
    "        \n",
    "        # (batch, h, seq_len, seq_len) --> (batch, h, seq_len, d_k)\n",
    "        # return attention scores which can be used for visualization\n",
    "        return (attention_scores @ value), attention_scores\n",
    "\n",
    "    def forward(self, q):\n",
    "        query = self.w_q(q) # (batch, seq_len, d_model) --> (batch, seq_len, d_model)\n",
    "        key = self.w_k(q) # (batch, seq_len, d_model) --> (batch, seq_len, d_model)\n",
    "        value = self.w_v(q) # (batch, seq_len, d_model) --> (batch, seq_len, d_model)\n",
    "\n",
    "        # (batch, seq_len, d_model) --> (batch, seq_len, h, d_k) --> (batch, h, seq_len, d_k)\n",
    "        query = query.view(query.shape[0], query.shape[1], self.H, self.dim_head).transpose(1, 2)\n",
    "        key = key.view(key.shape[0], key.shape[1], self.H, self.dim_head).transpose(1, 2)\n",
    "        value = value.view(value.shape[0], value.shape[1], self.H, self.dim_head).transpose(1, 2)\n",
    "\n",
    "        # Calculate attention\n",
    "        x, self.attention_scores = Visual_SpatialMultiHeadAttention.attention(query, key, value)\n",
    "        \n",
    "        # Combine all the heads together\n",
    "        # (batch, h, seq_len, d_k) --> (batch, seq_len, h, d_k) --> (batch, seq_len, d_model)\n",
    "        x = x.transpose(1, 2).contiguous().view(x.shape[0], -1, self.H * self.dim_head)\n",
    "\n",
    "        # Multiply by Wo\n",
    "        # (batch, seq_len, d_model) --> (batch, seq_len, d_model)  \n",
    "        return self.w_o(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Visual_ChannelMultiHeadAttention(nn.Module):\n",
    "    def __init__(self, s: int, H: int) -> None:\n",
    "        super().__init__()\n",
    "        self.s = s\n",
    "        self.H = H\n",
    "\n",
    "        #making sure s is divisible by  H, otherwise problem!\n",
    "        assert s%H == 0, \"dimensions of model are divisble by number of heads\"\n",
    "\n",
    "        self.dim_head = s // H\n",
    "        self.w_q = nn.Linear(s, s, bias = False)\n",
    "        self.w_k = nn.Linear(s, s, bias = False)\n",
    "        self.w_v = nn.Linear(s, s, bias = False)\n",
    "        self.w_o = nn.Linear(s, s, bias = False)\n",
    "\n",
    "    @staticmethod\n",
    "    def attention(query, key, value):\n",
    "        dim_head = query.shape[-1]\n",
    "        # Just apply the formula from the paper\n",
    "        # (batch, h, seq_len, d_k) --> (batch, h, seq_len, seq_len)\n",
    "        attention_scores = (query @ key.transpose(-2, -1)) / math.sqrt(dim_head)\n",
    "        attention_scores = attention_scores.softmax(dim=-1) # (batch, h, seq_len, seq_len) # Apply softmax\n",
    "        \n",
    "        # (batch, h, seq_len, seq_len) --> (batch, h, seq_len, d_k)\n",
    "        # return attention scores which can be used for visualization\n",
    "        return (attention_scores @ value), attention_scores\n",
    "\n",
    "    def forward(self, q):\n",
    "        query = self.w_q(q) # (batch, seq_len, d_model) --> (batch, seq_len, d_model)\n",
    "        key = self.w_k(q) # (batch, seq_len, d_model) --> (batch, seq_len, d_model)\n",
    "        value = self.w_v(q) # (batch, seq_len, d_model) --> (batch, seq_len, d_model)\n",
    "\n",
    "        # (batch, seq_len, d_model) --> (batch, seq_len, h, d_k) --> (batch, h, seq_len, d_k)\n",
    "        query = query.view(query.shape[0], query.shape[1], self.H, self.dim_head).transpose(1, 2)\n",
    "        key = key.view(key.shape[0], key.shape[1], self.H, self.dim_head).transpose(1, 2)\n",
    "        value = value.view(value.shape[0], value.shape[1], self.H, self.dim_head).transpose(1, 2)\n",
    "\n",
    "        # Calculate attention\n",
    "        x, self.attention_scores = Visual_ChannelMultiHeadAttention.attention(query, key, value)\n",
    "        \n",
    "        # Combine all the heads together\n",
    "        # (batch, h, seq_len, d_k) --> (batch, seq_len, h, d_k) --> (batch, seq_len, d_model)\n",
    "        x = x.transpose(1, 2).contiguous().view(x.shape[0], -1, self.H * self.dim_head)\n",
    "\n",
    "        # Multiply by Wo\n",
    "        # (batch, seq_len, d_model) --> (batch, seq_len, d_model)  \n",
    "        return self.w_o(x)\n",
    "        \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Visual_TemporalMultiHeadAttention(nn.Module):\n",
    "    def __init__(self, m: int, H: int) -> None:\n",
    "        super().__init__()\n",
    "        self.H = H\n",
    "        self.m = m\n",
    "\n",
    "        #making sure s is divisible by  H, otherwise problem!\n",
    "        assert m%H == 0, \"dimensions of model are divisble by number of heads\"\n",
    "\n",
    "        self.dim_head = m // H\n",
    "        self.w_q = nn.Linear(m, m, bias = False)\n",
    "        self.w_k = nn.Linear(m, m, bias = False)\n",
    "        self.w_v = nn.Linear(m, m, bias = False)\n",
    "        self.w_o = nn.Linear(m, self.dim_head*2, bias = False)\n",
    "\n",
    "    @staticmethod\n",
    "    def attention(query, key, value):\n",
    "        dim_head = query.shape[-1]\n",
    "        # Just apply the formula from the paper\n",
    "        # (batch, h, seq_len, d_k) --> (batch, h, seq_len, seq_len)\n",
    "        attention_scores = (query @ key.transpose(-2, -1)) / math.sqrt(dim_head)\n",
    "        attention_scores = attention_scores.softmax(dim=-1) # (batch, h, seq_len, seq_len) # Apply softmax\n",
    "        \n",
    "        # (batch, h, seq_len, seq_len) --> (batch, h, seq_len, d_k)\n",
    "        # return attention scores which can be used for visualization\n",
    "        return (attention_scores @ value), attention_scores\n",
    "\n",
    "    def forward(self, q):\n",
    "        query = self.w_q(q) # (batch, seq_len, d_model) --> (batch, seq_len, d_model)\n",
    "        key = self.w_k(q) # (batch, seq_len, d_model) --> (batch, seq_len, d_model)\n",
    "        value = self.w_v(q) # (batch, seq_len, d_model) --> (batch, seq_len, d_model)\n",
    "\n",
    "        # (batch, seq_len, d_model) --> (batch, seq_len, h, d_k) --> (batch, h, seq_len, d_k)\n",
    "        query = query.view(query.shape[0], query.shape[1], self.H, self.dim_head).transpose(1, 2)\n",
    "        key = key.view(key.shape[0], key.shape[1], self.H, self.dim_head).transpose(1, 2)\n",
    "        value = value.view(value.shape[0], value.shape[1], self.H, self.dim_head).transpose(1, 2)\n",
    "\n",
    "        # Calculate attention\n",
    "        x, self.attention_scores = Visual_TemporalMultiHeadAttention.attention(query, key, value)\n",
    "        \n",
    "        # Combine all the heads together\n",
    "        # (batch, h, seq_len, d_k) --> (batch, seq_len, h, d_k) --> (batch, seq_len, d_model)\n",
    "        x = x.transpose(1, 2).contiguous().view(x.shape[0], -1, self.H * self.dim_head)\n",
    "\n",
    "        # Multiply by Wo\n",
    "        # (batch, seq_len, d_model) --> (batch, seq_len, d_model)  \n",
    "        return self.w_o(x)\n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpatialAveragePooling(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SpatialAveragePooling, self).__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        #dimension of x --> N x m x s\n",
    "        pooled = torch.mean(x, dim=2)\n",
    "        return pooled\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Visual_BeforeCross(nn.Module):\n",
    "    def __init__(self, spatial: Visual_SpatialMultiHeadAttention, channel: Visual_ChannelMultiHeadAttention, sap: SpatialAveragePooling, temporal:Visual_TemporalMultiHeadAttention) -> None:\n",
    "        super().__init__()\n",
    "        self.spatial_selfAttention = spatial\n",
    "        self.channel_selfAttention = channel\n",
    "        self.pool = sap\n",
    "        self.temporal_selfAttention = temporal\n",
    "\n",
    "        #making sure s is divisible by  H, otherwise problem!\n",
    "        # assert m%H == 0, \"dimensions of model are divisble by number of heads\"\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.spatial_selfAttention(x)\n",
    "        print(\"After Spatial\")\n",
    "        print(x.shape)\n",
    "        x = x.transpose(1,2)\n",
    "        x = self.channel_selfAttention(x)\n",
    "        print(\"After Channel\")\n",
    "        print(x.shape)\n",
    "        x = self.pool(x)\n",
    "        print(\"After pool\")\n",
    "        print(x.shape)\n",
    "        x = x.unsqueeze(1)\n",
    "        x = self.temporal_selfAttention(x)\n",
    "        print(\"After Temporal\")\n",
    "        x= x.squeeze(1)\n",
    "        print(x.shape)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = 2048\n",
    "s = 64\n",
    "H = 8\n",
    "\n",
    "input_tensor = torch.randn(6, s, m)\n",
    "\n",
    "spatialSA = Visual_SpatialMultiHeadAttention(m, H)\n",
    "\n",
    "channelSA = Visual_ChannelMultiHeadAttention(s, H)\n",
    "\n",
    "sap = SpatialAveragePooling() \n",
    "\n",
    "temporalSA = Visual_TemporalMultiHeadAttention(m, H)\n",
    "\n",
    "v = Visual_BeforeCross(spatialSA, channelSA, sap, temporalSA)\n",
    "output_tensor = v.forward(input_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AV_CrossAttention(nn.Module):\n",
    "    def __init__(self, m: int) -> None:\n",
    "        super().__init__()\n",
    "        self.m = m\n",
    "        self.w = nn.Linear(m, m, bias = False)\n",
    "\n",
    "    def forward(self, audio_data, video_data):\n",
    "        Corr =  self.w(audio_data) @ video_data.transpose(0,1) # N*m * m*m * m*N -> N*N\n",
    "        \n",
    "        w_audio = F.softmax(Corr, dim =0)\n",
    "        w_video = F.softmax(Corr.transpose(0,1), dim = 0)\n",
    "        Dvideo = w_video @ video_data\n",
    "        Daudio = w_audio @ audio_data\n",
    "        \n",
    "        DCorrVideo = torch.tanh(Dvideo + video_data)\n",
    "        DCorrAudio = torch.tanh(Daudio + audio_data)\n",
    "       \n",
    "        return torch.cat((DCorrVideo,DCorrAudio), dim = 1)\n",
    "        \n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "N = 6\n",
    "m = 512\n",
    "audio_tensor = torch.randn(N, m)\n",
    "video_tensor = torch.randn(N, m)\n",
    "avCA = AV_CrossAttention(m)\n",
    "output = avCA(audio_tensor, video_tensor)\n",
    "print(output.shape)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flattened_output = output.view(-1)\n",
    "features = flattened_output.shape[0]\n",
    "\n",
    "out_classes = 8\n",
    "\n",
    "fc_layer = nn.Linear(features, out_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Visual_SpatialMultiHeadAttention(nn.Module):\n",
    "    def __init__(self, m: int, H: int) -> None:\n",
    "        super().__init__()\n",
    "        self.m = m\n",
    "        self.H = H\n",
    "        \n",
    "        #making sure m is divisible by  H, otherwise problem!\n",
    "        assert m % H == 0, \"dimensions of model are divisible by number of heads\"\n",
    "        \n",
    "        self.dim_head = m // H\n",
    "        self.w_q = nn.Linear(m, m, bias = False)\n",
    "        self.w_k = nn.Linear(m, m, bias = False)\n",
    "        self.w_v = nn.Linear(m, m, bias = False)\n",
    "        self.w_o = nn.Linear(m, m, bias = False)\n",
    "        \n",
    "    \n",
    "    @staticmethod\n",
    "    def attention(query, key, value):\n",
    "        dim_head = query.shape[-1]\n",
    "        # Just apply the formula from the paper\n",
    "        # (batch, h, seq_len, d_k) --> (batch, h, seq_len, seq_len)\n",
    "        attention_scores = (query @ key.transpose(-2, -1)) / math.sqrt(dim_head)\n",
    "        attention_scores = attention_scores.softmax(dim=-1) # (batch, h, seq_len, seq_len) # Apply softmax\n",
    "        \n",
    "        # (batch, h, seq_len, seq_len) --> (batch, h, seq_len, d_k)\n",
    "        # return attention scores which can be used for visualization\n",
    "        return (attention_scores @ value), attention_scores\n",
    "\n",
    "    def forward(self, q):\n",
    "        query = self.w_q(q) # (batch, seq_len, d_model) --> (batch, seq_len, d_model)\n",
    "        key = self.w_k(q) # (batch, seq_len, d_model) --> (batch, seq_len, d_model)\n",
    "        value = self.w_v(q) # (batch, seq_len, d_model) --> (batch, seq_len, d_model)\n",
    "\n",
    "        # (batch, seq_len, d_model) --> (batch, seq_len, h, d_k) --> (batch, h, seq_len, d_k)\n",
    "        query = query.view(query.shape[0], query.shape[1], self.H, self.dim_head).transpose(1, 2)\n",
    "        key = key.view(key.shape[0], key.shape[1], self.H, self.dim_head).transpose(1, 2)\n",
    "        value = value.view(value.shape[0], value.shape[1], self.H, self.dim_head).transpose(1, 2)\n",
    "\n",
    "        # Calculate attention\n",
    "        x, self.attention_scores = Visual_SpatialMultiHeadAttention.attention(query, key, value)\n",
    "        \n",
    "        # Combine all the heads together\n",
    "        # (batch, h, seq_len, d_k) --> (batch, seq_len, h, d_k) --> (batch, seq_len, d_model)\n",
    "        x = x.transpose(1, 2).contiguous().view(x.shape[0], -1, self.H * self.dim_head)\n",
    "\n",
    "        # Multiply by Wo\n",
    "        # (batch, seq_len, d_model) --> (batch, seq_len, d_model)  \n",
    "        return self.w_o(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Visual_TemporalMultiHeadAttention(nn.Module):\n",
    "    def __init__(self, m: int, H: int) -> None:\n",
    "        super().__init__()\n",
    "        self.H = H\n",
    "        self.m = m\n",
    "\n",
    "        #making sure s is divisible by  H, otherwise problem!\n",
    "        assert m%H == 0, \"dimensions of model are divisble by number of heads\"\n",
    "\n",
    "        self.dim_head = m // H\n",
    "        self.w_q = nn.Linear(m, m, bias = False)\n",
    "        self.w_k = nn.Linear(m, m, bias = False)\n",
    "        self.w_v = nn.Linear(m, m, bias = False)\n",
    "        self.w_o = nn.Linear(m, self.dim_head*2, bias = False)\n",
    "\n",
    "    @staticmethod\n",
    "    def attention(query, key, value):\n",
    "        dim_head = query.shape[-1]\n",
    "        # Just apply the formula from the paper\n",
    "        # (batch, h, seq_len, d_k) --> (batch, h, seq_len, seq_len)\n",
    "        attention_scores = (query @ key.transpose(-2, -1)) / math.sqrt(dim_head)\n",
    "        attention_scores = attention_scores.softmax(dim=-1) # (batch, h, seq_len, seq_len) # Apply softmax\n",
    "        \n",
    "        # (batch, h, seq_len, seq_len) --> (batch, h, seq_len, d_k)\n",
    "        # return attention scores which can be used for visualization\n",
    "        return (attention_scores @ value), attention_scores\n",
    "\n",
    "    def forward(self, q):\n",
    "        query = self.w_q(q) # (batch, seq_len, d_model) --> (batch, seq_len, d_model)\n",
    "        key = self.w_k(q) # (batch, seq_len, d_model) --> (batch, seq_len, d_model)\n",
    "        value = self.w_v(q) # (batch, seq_len, d_model) --> (batch, seq_len, d_model)\n",
    "\n",
    "        # (batch, seq_len, d_model) --> (batch, seq_len, h, d_k) --> (batch, h, seq_len, d_k)\n",
    "        query = query.view(query.shape[0], query.shape[1], self.H, self.dim_head).transpose(1, 2)\n",
    "        key = key.view(key.shape[0], key.shape[1], self.H, self.dim_head).transpose(1, 2)\n",
    "        value = value.view(value.shape[0], value.shape[1], self.H, self.dim_head).transpose(1, 2)\n",
    "\n",
    "        # Calculate attention\n",
    "        x, self.attention_scores = Visual_TemporalMultiHeadAttention.attention(query, key, value)\n",
    "        \n",
    "        # Combine all the heads together\n",
    "        # (batch, h, seq_len, d_k) --> (batch, seq_len, h, d_k) --> (batch, seq_len, d_model)\n",
    "        x = x.transpose(1, 2).contiguous().view(x.shape[0], -1, self.H * self.dim_head)\n",
    "\n",
    "        # Multiply by Wo\n",
    "        # (batch, seq_len, d_model) --> (batch, seq_len, d_model)  \n",
    "        return self.w_o(x)\n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Visual_BeforeCross(nn.Module):\n",
    "    def __init__(self, spatial: Visual_SpatialMultiHeadAttention, channel: Visual_ChannelMultiHeadAttention, sap: SpatialAveragePooling, temporal:Visual_TemporalMultiHeadAttention) -> None:\n",
    "        super().__init__()\n",
    "        self.spatial_selfAttention = spatial\n",
    "        self.channel_selfAttention = channel\n",
    "        self.pool = sap\n",
    "        self.temporal_selfAttention = temporal\n",
    "\n",
    "        #making sure s is divisible by  H, otherwise problem!\n",
    "        # assert m%H == 0, \"dimensions of model are divisble by number of heads\"\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.spatial_selfAttention(x)\n",
    "        print(\"After Spatial\")\n",
    "        print(x.shape)\n",
    "        x = x.transpose(1,2)\n",
    "        x = self.channel_selfAttention(x)\n",
    "        print(\"After Channel\")\n",
    "        print(x.shape)\n",
    "        x = self.pool(x)\n",
    "        print(\"After pool\")\n",
    "        print(x.shape)\n",
    "        x = x.unsqueeze(1)\n",
    "        x = self.temporal_selfAttention(x)\n",
    "        print(\"After Temporal\")\n",
    "        x= x.squeeze(1)\n",
    "        print(x.shape)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AV_CrossAttention(nn.Module):\n",
    "    def __init__(self, m: int) -> None:\n",
    "        super().__init__()\n",
    "        self.m = m\n",
    "        self.w = nn.Linear(m, m, bias = False)\n",
    "\n",
    "    def forward(self, audio_data, video_data):\n",
    "        Corr =  self.w(audio_data) @ video_data.transpose(0,1) # N*m * m*m * m*N -> N*N\n",
    "        \n",
    "        w_audio = F.softmax(Corr, dim =0)\n",
    "        w_video = F.softmax(Corr.transpose(0,1), dim = 0)\n",
    "        Dvideo = w_video @ video_data\n",
    "        Daudio = w_audio @ audio_data\n",
    "        \n",
    "        DCorrVideo = torch.tanh(Dvideo + video_data)\n",
    "        DCorrAudio = torch.tanh(Daudio + audio_data)\n",
    "       \n",
    "        return torch.cat((DCorrVideo,DCorrAudio), dim = 1)\n",
    "        \n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "N = 6\n",
    "m = 512\n",
    "audio_tensor = torch.randn(N, m)\n",
    "video_tensor = torch.randn(N, m)\n",
    "avCA = AV_CrossAttention(m)\n",
    "output = avCA(audio_tensor, video_tensor)\n",
    "print(output.shape)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Visual_ChannelMultiHeadAttention(nn.Module):\n",
    "    def __init__(self, s: int, H: int) -> None:\n",
    "        super().__init__()\n",
    "        self.s = s\n",
    "        self.H = H\n",
    "\n",
    "        #making sure s is divisible by  H, otherwise problem!\n",
    "        assert s%H == 0, \"dimensions of model are divisble by number of heads\"\n",
    "\n",
    "        self.dim_head = s // H\n",
    "        self.w_q = nn.Linear(s, s, bias = False)\n",
    "        self.w_k = nn.Linear(s, s, bias = False)\n",
    "        self.w_v = nn.Linear(s, s, bias = False)\n",
    "        self.w_o = nn.Linear(s, s, bias = False)\n",
    "\n",
    "    @staticmethod\n",
    "    def attention(query, key, value):\n",
    "        dim_head = query.shape[-1]\n",
    "        # Just apply the formula from the paper\n",
    "        # (batch, h, seq_len, d_k) --> (batch, h, seq_len, seq_len)\n",
    "        attention_scores = (query @ key.transpose(-2, -1)) / math.sqrt(dim_head)\n",
    "        attention_scores = attention_scores.softmax(dim=-1) # (batch, h, seq_len, seq_len) # Apply softmax\n",
    "        \n",
    "        # (batch, h, seq_len, seq_len) --> (batch, h, seq_len, d_k)\n",
    "        # return attention scores which can be used for visualization\n",
    "        return (attention_scores @ value), attention_scores\n",
    "\n",
    "    def forward(self, q):\n",
    "        query = self.w_q(q) # (batch, seq_len, d_model) --> (batch, seq_len, d_model)\n",
    "        key = self.w_k(q) # (batch, seq_len, d_model) --> (batch, seq_len, d_model)\n",
    "        value = self.w_v(q) # (batch, seq_len, d_model) --> (batch, seq_len, d_model)\n",
    "\n",
    "        # (batch, seq_len, d_model) --> (batch, seq_len, h, d_k) --> (batch, h, seq_len, d_k)\n",
    "        query = query.view(query.shape[0], query.shape[1], self.H, self.dim_head).transpose(1, 2)\n",
    "        key = key.view(key.shape[0], key.shape[1], self.H, self.dim_head).transpose(1, 2)\n",
    "        value = value.view(value.shape[0], value.shape[1], self.H, self.dim_head).transpose(1, 2)\n",
    "\n",
    "        # Calculate attention\n",
    "        x, self.attention_scores = Visual_ChannelMultiHeadAttention.attention(query, key, value)\n",
    "        \n",
    "        # Combine all the heads together\n",
    "        # (batch, h, seq_len, d_k) --> (batch, seq_len, h, d_k) --> (batch, seq_len, d_model)\n",
    "        x = x.transpose(1, 2).contiguous().view(x.shape[0], -1, self.H * self.dim_head)\n",
    "\n",
    "        # Multiply by Wo\n",
    "        # (batch, seq_len, d_model) --> (batch, seq_len, d_model)  \n",
    "        return self.w_o(x)\n",
    "        \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpatialAveragePooling(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SpatialAveragePooling, self).__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        #dimension of x --> N x m x s\n",
    "        pooled = torch.mean(x, dim=2)\n",
    "        return pooled\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = 2048\n",
    "s = 64\n",
    "H = 8\n",
    "\n",
    "input_tensor = torch.randn(6, s, m)\n",
    "\n",
    "spatialSA = Visual_SpatialMultiHeadAttention(m, H)\n",
    "\n",
    "channelSA = Visual_ChannelMultiHeadAttention(s, H)\n",
    "\n",
    "sap = SpatialAveragePooling() \n",
    "\n",
    "temporalSA = Visual_TemporalMultiHeadAttention(m, H)\n",
    "\n",
    "v = Visual_BeforeCross(spatialSA, channelSA, sap, temporalSA)\n",
    "output_tensor = v.forward(input_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flattened_output = output.view(-1)\n",
    "features = flattened_output.shape[0]\n",
    "\n",
    "out_classes = 8\n",
    "\n",
    "fc_layer = nn.Linear(features, out_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Visual_SpatialMultiHeadAttention(nn.Module):\n",
    "    def __init__(self, m: int, H: int) -> None:\n",
    "        super().__init__()\n",
    "        self.m = m\n",
    "        self.H = H\n",
    "        \n",
    "        #making sure m is divisible by  H, otherwise problem!\n",
    "        assert m % H == 0, \"dimensions of model are divisible by number of heads\"\n",
    "        \n",
    "        self.dim_head = m // H\n",
    "        self.w_q = nn.Linear(m, m, bias = False)\n",
    "        self.w_k = nn.Linear(m, m, bias = False)\n",
    "        self.w_v = nn.Linear(m, m, bias = False)\n",
    "        self.w_o = nn.Linear(m, m, bias = False)\n",
    "        \n",
    "    \n",
    "    @staticmethod\n",
    "    def attention(query, key, value):\n",
    "        dim_head = query.shape[-1]\n",
    "        # Just apply the formula from the paper\n",
    "        # (batch, h, seq_len, d_k) --> (batch, h, seq_len, seq_len)\n",
    "        attention_scores = (query @ key.transpose(-2, -1)) / math.sqrt(dim_head)\n",
    "        attention_scores = attention_scores.softmax(dim=-1) # (batch, h, seq_len, seq_len) # Apply softmax\n",
    "        \n",
    "        # (batch, h, seq_len, seq_len) --> (batch, h, seq_len, d_k)\n",
    "        # return attention scores which can be used for visualization\n",
    "        return (attention_scores @ value), attention_scores\n",
    "\n",
    "    def forward(self, q):\n",
    "        query = self.w_q(q) # (batch, seq_len, d_model) --> (batch, seq_len, d_model)\n",
    "        key = self.w_k(q) # (batch, seq_len, d_model) --> (batch, seq_len, d_model)\n",
    "        value = self.w_v(q) # (batch, seq_len, d_model) --> (batch, seq_len, d_model)\n",
    "\n",
    "        # (batch, seq_len, d_model) --> (batch, seq_len, h, d_k) --> (batch, h, seq_len, d_k)\n",
    "        query = query.view(query.shape[0], query.shape[1], self.H, self.dim_head).transpose(1, 2)\n",
    "        key = key.view(key.shape[0], key.shape[1], self.H, self.dim_head).transpose(1, 2)\n",
    "        value = value.view(value.shape[0], value.shape[1], self.H, self.dim_head).transpose(1, 2)\n",
    "\n",
    "        # Calculate attention\n",
    "        x, self.attention_scores = Visual_SpatialMultiHeadAttention.attention(query, key, value)\n",
    "        \n",
    "        # Combine all the heads together\n",
    "        # (batch, h, seq_len, d_k) --> (batch, seq_len, h, d_k) --> (batch, seq_len, d_model)\n",
    "        x = x.transpose(1, 2).contiguous().view(x.shape[0], -1, self.H * self.dim_head)\n",
    "\n",
    "        # Multiply by Wo\n",
    "        # (batch, seq_len, d_model) --> (batch, seq_len, d_model)  \n",
    "        return self.w_o(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Visual_BeforeCross(nn.Module):\n",
    "    def __init__(self, spatial: Visual_SpatialMultiHeadAttention, channel: Visual_ChannelMultiHeadAttention, sap: SpatialAveragePooling, temporal:Visual_TemporalMultiHeadAttention) -> None:\n",
    "        super().__init__()\n",
    "        self.spatial_selfAttention = spatial\n",
    "        self.channel_selfAttention = channel\n",
    "        self.pool = sap\n",
    "        self.temporal_selfAttention = temporal\n",
    "\n",
    "        #making sure s is divisible by  H, otherwise problem!\n",
    "        # assert m%H == 0, \"dimensions of model are divisble by number of heads\"\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.spatial_selfAttention(x)\n",
    "        print(\"After Spatial\")\n",
    "        print(x.shape)\n",
    "        x = x.transpose(1,2)\n",
    "        x = self.channel_selfAttention(x)\n",
    "        print(\"After Channel\")\n",
    "        print(x.shape)\n",
    "        x = self.pool(x)\n",
    "        print(\"After pool\")\n",
    "        print(x.shape)\n",
    "        x = x.unsqueeze(1)\n",
    "        x = self.temporal_selfAttention(x)\n",
    "        print(\"After Temporal\")\n",
    "        x= x.squeeze(1)\n",
    "        print(x.shape)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[-1.5324,  0.6976,  0.4354, -0.2854],\n",
      "          [ 0.5968, -0.2856,  1.1082, -1.3365],\n",
      "          [-0.0670,  2.0308, -0.3248, -0.3032]],\n",
      "\n",
      "         [[-0.2464,  0.8150, -0.3019, -0.3060],\n",
      "          [ 0.9260,  0.7585,  1.3252,  0.6573],\n",
      "          [ 3.6754, -0.2269, -0.2730, -0.3768]]]])\n",
      "tensor([[-0.2737,  0.5084,  0.3569, -0.0329,  0.5556],\n",
      "        [-0.8872,  0.5788, -0.9865,  0.2042, -0.5409],\n",
      "        [-0.2331,  1.9347,  0.3152,  1.2046, -0.4526],\n",
      "        [-0.1262, -0.2897, -1.2247, -1.4188, -0.3763]])\n",
      "torch.Size([1, 2, 3, 5])\n",
      "tensor([[[[-2.6496e-01,  5.4976e-01, -7.4832e-01,  1.1223e+00, -1.3184e+00],\n",
      "          [ 5.4759e-04,  2.6693e+00,  2.4808e+00,  3.1531e+00,  4.8746e-01],\n",
      "          [-1.6694e+00,  6.0084e-01, -1.7584e+00,  4.5579e-01, -8.7468e-01]],\n",
      "\n",
      "         [[-5.4661e-01, -1.4900e-01, -6.1234e-01,  2.4494e-01, -3.2597e-01],\n",
      "          [-1.3182e+00,  3.2831e+00, -8.0503e-01,  7.8822e-01, -7.4291e-01],\n",
      "          [-6.9340e-01,  1.3183e+00,  1.9109e+00,  3.8389e-02,  2.4300e+00]]]])\n"
     ]
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
