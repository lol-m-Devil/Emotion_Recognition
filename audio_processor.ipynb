{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 1: Extract Audio Files from Video Clips in another folder: Audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from moviepy.editor import VideoFileClip\n",
    "import os\n",
    "\n",
    "def extract_audio_from_folder(input_folder, output_folder):\n",
    "    # Create the output folder if it doesn't exist\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "    # Process each video file in the input folder\n",
    "    for video_file in os.listdir(input_folder):\n",
    "        if video_file.endswith('.mp4'):\n",
    "            input_video_path = os.path.join(input_folder, video_file)\n",
    "            output_audio_path = os.path.join(output_folder, f'{os.path.splitext(video_file)[0]}.wav')\n",
    "\n",
    "            extract_audio(input_video_path, output_audio_path)\n",
    "\n",
    "def extract_audio(input_video_path, output_audio_path):\n",
    "    video_clip = VideoFileClip(input_video_path)\n",
    "    audio_clip = video_clip.audio\n",
    "\n",
    "    audio_clip.write_audiofile(output_audio_path, codec='pcm_s16le', fps=audio_clip.fps)\n",
    "\n",
    "    video_clip.close()\n",
    "\n",
    "# Example usage\n",
    "input_video_folder = 'Single_Actor_01'\n",
    "output_audio_folder = 'Single_Audio_01'\n",
    "\n",
    "extract_audio_from_folder(input_video_folder, output_audio_folder)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 2: Generate Normalized Spectograms from Audio Files present in folder: Audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "import librosa.display\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "def generate_spectrogram(audio_file, fft_size=256, hop_size=10, window_size=32, num_parts=6):\n",
    "    # Load audio file\n",
    "    y, sr = librosa.load(audio_file, sr=None, duration=4)\n",
    "\n",
    "    # Calculate the required padding for the spectrogram\n",
    "    n_fft = fft_size\n",
    "    hop_length = int(sr * hop_size / 1000)  # Convert hop_size from ms to samples\n",
    "    win_length = int(sr * window_size / 1000)  # Convert window_size from ms to samples\n",
    "\n",
    "    # Adjust the n_fft to be at least the length of the signal\n",
    "    n_fft = max(n_fft, len(y))\n",
    "\n",
    "    # Compute spectrogram\n",
    "    spectrogram = librosa.feature.melspectrogram(\n",
    "        y=y,\n",
    "        sr=sr,\n",
    "        n_fft=n_fft,\n",
    "        hop_length=hop_length,\n",
    "        win_length=win_length,\n",
    "        window=\"hamming\",\n",
    "        n_mels=256  # Number of frequency components\n",
    "    )\n",
    "\n",
    "    # Convert to decibels\n",
    "    spectrogram_db = librosa.power_to_db(spectrogram, ref=np.max)\n",
    "\n",
    "    # Split spectrogram into N shorter parts\n",
    "    part_size = spectrogram_db.shape[1] // num_parts\n",
    "    spectrogram_parts = [spectrogram_db[:, i * part_size:(i + 1) * part_size] for i in range(num_parts)]\n",
    "\n",
    "    return spectrogram_parts\n",
    "\n",
    "def normalize_sequences(sequences):\n",
    "    # Flatten the sequences to compute mean and variance\n",
    "    flat_sequences = np.concatenate(sequences, axis=1)\n",
    "\n",
    "    # Compute mean and variance\n",
    "    mean = np.mean(flat_sequences, axis=1, keepdims=True)\n",
    "    std = np.std(flat_sequences, axis=1, keepdims=True)\n",
    "\n",
    "    # Normalize sequences\n",
    "    normalized_sequences = [(seq - mean) / std for seq in sequences]\n",
    "\n",
    "    return normalized_sequences\n",
    "    \n",
    "def save_normalized_spectrogram_images(audio_file_path, audio_file,normalized_parts, output_folder):\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "    y, sr = librosa.load(audio_file_path, sr=None, duration=4)\n",
    "    hop_length = int(sr * 10 / 1000)\n",
    "    \n",
    "    for i, part in enumerate(normalized_parts):\n",
    "        # Plot the normalized spectrogram without labels\n",
    "        plt.figure(figsize=(6, 4))\n",
    "        librosa.display.specshow(part, sr=sr, hop_length=hop_length, x_axis=None, y_axis=None)\n",
    "        plt.axis('off')\n",
    "\n",
    "        # Save the image\n",
    "        image_path = os.path.join(output_folder, f'{os.path.splitext(audio_file)[0]}-0{i+1}.png')\n",
    "        plt.savefig(image_path, bbox_inches='tight', pad_inches=0)\n",
    "        plt.close()\n",
    "\n",
    "def save_normalized_spectrogram_images_from_folder(input_folder, output_folder):\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "    # Process each audio file in the input folder\n",
    "    for audio_file in os.listdir(input_folder):\n",
    "        if audio_file.endswith('.wav'):\n",
    "            audio_file_path = os.path.join(input_folder, audio_file)\n",
    "            spectrogram_parts = generate_spectrogram(audio_file_path)\n",
    "            normalized_parts = normalize_sequences(spectrogram_parts)\n",
    "            save_normalized_spectrogram_images(audio_file_path, audio_file, normalized_parts, output_folder)\n",
    "            \n",
    "\n",
    "# Example usage\n",
    "input_audio_folder = 'Single_Audio_01'\n",
    "output_spectrogram_folder = 'Single_Spectogram_01'\n",
    "\n",
    "save_normalized_spectrogram_images_from_folder(input_audio_folder, output_spectrogram_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 3: Get the output from resnet-18 and also apply spatial pooling afterwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "import torchvision.models as models\n",
    "import torch.nn as nn\n",
    "import os\n",
    "import warnings\n",
    "\n",
    "# # Suppress all warnings\n",
    "# warnings.filterwarnings(\"ignore\")\n",
    "# resnet18 = torch.hub.load('pytorch/vision:v0.10.0', 'resnet18', pretrained=True)\n",
    "# warnings.resetwarnings()\n",
    "# # Remove global average pooling layer\n",
    "# resnet18 = nn.Sequential(*list(resnet18.children())[:-1])\n",
    "\n",
    "\n",
    "# Load the pretrained ResNet-18 model\n",
    "resnet18 = models.resnet18(pretrained=True)\n",
    "print(resnet18)\n",
    "# Get the features part of the ResNet-18 model\n",
    "features = list(resnet18.children())[:-2]  # Remove the last two layers (average pooling and fully connected)\n",
    "\n",
    "# Create a new model without the global average pooling and fully connected layers\n",
    "resnet18_without_top = nn.Sequential(*features)\n",
    "print(resnet18_without_top)\n",
    "# Optionally, set the new model to evaluation mode\n",
    "resnet18_without_top.eval()\n",
    "# Add spatial average pooling layer\n",
    "# resnet18.add_module('avgpool', nn.AdaptiveAvgPool2d(1))\n",
    "\n",
    "# resnet18.eval()\n",
    "\n",
    "\n",
    "# Define a transformation to preprocess the input image\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Lambda(lambda x: x[:3, :, :]),  # Remove alpha channel if present\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "# Folder containing images\n",
    "image_folder = 'Single_Spectogram_01'\n",
    "\n",
    "# List to store individual outputs\n",
    "All_audio_outputs = []\n",
    "\n",
    "# Process each image in the folder\n",
    "for filename in os.listdir(image_folder):\n",
    "    if filename.endswith('.png'):\n",
    "        image_path = os.path.join(image_folder, filename)\n",
    "        \n",
    "        # Load and preprocess the image\n",
    "        image = Image.open(image_path)\n",
    "        input_tensor = transform(image)\n",
    "        input_batch = input_tensor.unsqueeze(0)  # Add batch dimension\n",
    "\n",
    "        # Perform inference\n",
    "        with torch.no_grad():\n",
    "            audio_output = resnet18(input_batch)\n",
    "        \n",
    "        # Append the output to the list\n",
    "        All_audio_outputs.append(audio_output)\n",
    "\n",
    "All_audio_outputs = torch.stack(All_audio_outputs)\n",
    "print(All_audio_outputs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "from typing import Any, Callable, List, Optional, Type, Union\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import Tensor\n",
    "\n",
    "# from ..transforms._presets import ImageClassification\n",
    "# from ..utils import _log_api_usage_once\n",
    "# from ._api import register_model, Weights, WeightsEnum\n",
    "# from ._meta import _IMAGENET_CATEGORIES\n",
    "# from ._utils import _ovewrite_named_param, handle_legacy_interface\n",
    "\n",
    "def conv3x3(in_planes: int, out_planes: int, stride: int = 1, groups: int = 1, dilation: int = 1) -> nn.Conv2d:\n",
    "    \"\"\"3x3 convolution with padding\"\"\"\n",
    "    return nn.Conv2d(\n",
    "        in_planes,\n",
    "        out_planes,\n",
    "        kernel_size=3,\n",
    "        stride=stride,\n",
    "        padding=dilation,\n",
    "        groups=groups,\n",
    "        bias=False,\n",
    "        dilation=dilation,\n",
    "    )\n",
    "\n",
    "\n",
    "def conv1x1(in_planes: int, out_planes: int, stride: int = 1) -> nn.Conv2d:\n",
    "    \"\"\"1x1 convolution\"\"\"\n",
    "    return nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride, bias=False)\n",
    "\n",
    "\n",
    "class BasicBlock(nn.Module):\n",
    "    expansion: int = 1\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        inplanes: int,\n",
    "        planes: int,\n",
    "        stride: int = 1,\n",
    "        downsample: Optional[nn.Module] = None,\n",
    "        groups: int = 1,\n",
    "        base_width: int = 64,\n",
    "        dilation: int = 1,\n",
    "        norm_layer: Optional[Callable[..., nn.Module]] = None,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        if norm_layer is None:\n",
    "            norm_layer = nn.BatchNorm2d\n",
    "        if groups != 1 or base_width != 64:\n",
    "            raise ValueError(\"BasicBlock only supports groups=1 and base_width=64\")\n",
    "        if dilation > 1:\n",
    "            raise NotImplementedError(\"Dilation > 1 not supported in BasicBlock\")\n",
    "        # Both self.conv1 and self.downsample layers downsample the input when stride != 1\n",
    "        self.conv1 = conv3x3(inplanes, planes, stride)\n",
    "        self.bn1 = norm_layer(planes)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv2 = conv3x3(planes, planes)\n",
    "        self.bn2 = norm_layer(planes)\n",
    "        self.downsample = downsample\n",
    "        self.stride = stride\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        identity = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            identity = self.downsample(x)\n",
    "\n",
    "        out += identity\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "class SpatialAveragePooling(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SpatialAveragePooling, self).__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        #dimension of x --> N x m x s\n",
    "        pooled = torch.mean(x, dim=(2,3))\n",
    "        return pooled\n",
    "    \n",
    "class ResNet(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        block: Type[BasicBlock],\n",
    "        layers: List[int],\n",
    "        num_classes: int = 1000,\n",
    "        zero_init_residual: bool = False,\n",
    "        groups: int = 1,\n",
    "        width_per_group: int = 64,\n",
    "        replace_stride_with_dilation: Optional[List[bool]] = None,\n",
    "        norm_layer: Optional[Callable[..., nn.Module]] = None,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        #_log_api_usage_once(self)\n",
    "        if norm_layer is None:\n",
    "            norm_layer = nn.BatchNorm2d\n",
    "        self._norm_layer = norm_layer\n",
    "\n",
    "        self.inplanes = 64\n",
    "        self.dilation = 1\n",
    "        if replace_stride_with_dilation is None:\n",
    "            # each element in the tuple indicates if we should replace\n",
    "            # the 2x2 stride with a dilated convolution instead\n",
    "            replace_stride_with_dilation = [False, False, False]\n",
    "        if len(replace_stride_with_dilation) != 3:\n",
    "            raise ValueError(\n",
    "                \"replace_stride_with_dilation should be None \"\n",
    "                f\"or a 3-element tuple, got {replace_stride_with_dilation}\"\n",
    "            )\n",
    "        self.groups = groups\n",
    "        self.base_width = width_per_group\n",
    "        self.conv1 = nn.Conv2d(3, self.inplanes, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "        self.bn1 = norm_layer(self.inplanes)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "        self.layer1 = self._make_layer(block, 64, layers[0])\n",
    "        self.layer2 = self._make_layer(block, 128, layers[1], stride=2, dilate=replace_stride_with_dilation[0])\n",
    "        self.layer3 = self._make_layer(block, 256, layers[2], stride=2, dilate=replace_stride_with_dilation[1])\n",
    "        self.layer4 = self._make_layer(block, 512, layers[3], stride=2, dilate=replace_stride_with_dilation[2])\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.fc = nn.Linear(512 * block.expansion, num_classes)\n",
    "\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode=\"fan_out\", nonlinearity=\"relu\")\n",
    "            elif isinstance(m, (nn.BatchNorm2d, nn.GroupNorm)):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "        # Zero-initialize the last BN in each residual branch,\n",
    "        # so that the residual branch starts with zeros, and each residual block behaves like an identity.\n",
    "        # This improves the model by 0.2~0.3% according to https://arxiv.org/abs/1706.02677\n",
    "        if zero_init_residual:\n",
    "            for m in self.modules():\n",
    "                if isinstance(m, BasicBlock) and m.bn2.weight is not None:\n",
    "                    nn.init.constant_(m.bn2.weight, 0)  # type: ignore[arg-type]\n",
    "\n",
    "    def _make_layer(\n",
    "        self,\n",
    "        block: Type[Union[BasicBlock]],\n",
    "        planes: int,\n",
    "        blocks: int,\n",
    "        stride: int = 1,\n",
    "        dilate: bool = False,\n",
    "    ) -> nn.Sequential:\n",
    "        norm_layer = self._norm_layer\n",
    "        downsample = None\n",
    "        previous_dilation = self.dilation\n",
    "        if dilate:\n",
    "            self.dilation *= stride\n",
    "            stride = 1\n",
    "        if stride != 1 or self.inplanes != planes * block.expansion:\n",
    "            downsample = nn.Sequential(\n",
    "                conv1x1(self.inplanes, planes * block.expansion, stride),\n",
    "                norm_layer(planes * block.expansion),\n",
    "            )\n",
    "\n",
    "        layers = []\n",
    "        layers.append(\n",
    "            block(\n",
    "                self.inplanes, planes, stride, downsample, self.groups, self.base_width, previous_dilation, norm_layer\n",
    "            )\n",
    "        )\n",
    "        self.inplanes = planes * block.expansion\n",
    "        for _ in range(1, blocks):\n",
    "            layers.append(\n",
    "                block(\n",
    "                    self.inplanes,\n",
    "                    planes,\n",
    "                    groups=self.groups,\n",
    "                    base_width=self.base_width,\n",
    "                    dilation=self.dilation,\n",
    "                    norm_layer=norm_layer,\n",
    "                )\n",
    "            )\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def _forward_impl(self, x: Tensor) -> Tensor:\n",
    "        # See note [TorchScript super()]\n",
    "        x = self.conv1(x)\n",
    "       \n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool(x)\n",
    "              \n",
    "\n",
    "        x = self.layer1(x)\n",
    "        \n",
    "        x = self.layer2(x)\n",
    "        \n",
    "        x = self.layer3(x)\n",
    "        \n",
    "        x = self.layer4(x)\n",
    "       \n",
    "\n",
    "        # x = self.avgpool(x)\n",
    "        # x = torch.flatten(x, 1)\n",
    "        # x = self.fc(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        return self._forward_impl(x)\n",
    "\n",
    "\n",
    "def _resnet(\n",
    "    block: Type[BasicBlock],\n",
    "    layers: List[int],\n",
    "    **kwargs: Any,\n",
    ") -> ResNet:\n",
    "    # if weights is not None:\n",
    "    #     _ovewrite_named_param(kwargs, \"num_classes\", len(weights.meta[\"categories\"]))\n",
    "\n",
    "    model = ResNet(block, layers, **kwargs)\n",
    "\n",
    "    # if weights is not None:\n",
    "    #     model.load_state_dict(weights.get_state_dict(progress=progress, check_hash=True))\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After conv1\n",
      "torch.Size([1, 64, 112, 112])\n",
      "After Maxpool\n",
      "torch.Size([1, 64, 56, 56])\n",
      "After l1\n",
      "torch.Size([1, 64, 56, 56])\n",
      "After l2\n",
      "torch.Size([1, 128, 28, 28])\n",
      "After l3\n",
      "torch.Size([1, 256, 14, 14])\n",
      "After l4\n",
      "torch.Size([1, 512, 7, 7])\n",
      "After conv1\n",
      "torch.Size([1, 64, 112, 112])\n",
      "After Maxpool\n",
      "torch.Size([1, 64, 56, 56])\n",
      "After l1\n",
      "torch.Size([1, 64, 56, 56])\n",
      "After l2\n",
      "torch.Size([1, 128, 28, 28])\n",
      "After l3\n",
      "torch.Size([1, 256, 14, 14])\n",
      "After l4\n",
      "torch.Size([1, 512, 7, 7])\n",
      "After conv1\n",
      "torch.Size([1, 64, 112, 112])\n",
      "After Maxpool\n",
      "torch.Size([1, 64, 56, 56])\n",
      "After l1\n",
      "torch.Size([1, 64, 56, 56])\n",
      "After l2\n",
      "torch.Size([1, 128, 28, 28])\n",
      "After l3\n",
      "torch.Size([1, 256, 14, 14])\n",
      "After l4\n",
      "torch.Size([1, 512, 7, 7])\n",
      "After conv1\n",
      "torch.Size([1, 64, 112, 112])\n",
      "After Maxpool\n",
      "torch.Size([1, 64, 56, 56])\n",
      "After l1\n",
      "torch.Size([1, 64, 56, 56])\n",
      "After l2\n",
      "torch.Size([1, 128, 28, 28])\n",
      "After l3\n",
      "torch.Size([1, 256, 14, 14])\n",
      "After l4\n",
      "torch.Size([1, 512, 7, 7])\n",
      "After conv1\n",
      "torch.Size([1, 64, 112, 112])\n",
      "After Maxpool\n",
      "torch.Size([1, 64, 56, 56])\n",
      "After l1\n",
      "torch.Size([1, 64, 56, 56])\n",
      "After l2\n",
      "torch.Size([1, 128, 28, 28])\n",
      "After l3\n",
      "torch.Size([1, 256, 14, 14])\n",
      "After l4\n",
      "torch.Size([1, 512, 7, 7])\n",
      "After conv1\n",
      "torch.Size([1, 64, 112, 112])\n",
      "After Maxpool\n",
      "torch.Size([1, 64, 56, 56])\n",
      "After l1\n",
      "torch.Size([1, 64, 56, 56])\n",
      "After l2\n",
      "torch.Size([1, 128, 28, 28])\n",
      "After l3\n",
      "torch.Size([1, 256, 14, 14])\n",
      "After l4\n",
      "torch.Size([1, 512, 7, 7])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "import torchvision.models as models\n",
    "import torch.nn as nn\n",
    "import os\n",
    "import math\n",
    "import warnings\n",
    "\n",
    "model_path = 'resnet18-f37072fd.pth'\n",
    "checkpoint = torch.load(model_path, map_location=torch.device('cpu'))\n",
    "Resnet18 = _resnet(BasicBlock, [2, 2, 2, 2])\n",
    "\n",
    "Resnet18.load_state_dict(checkpoint)\n",
    "\n",
    "pooling_layer = SpatialAveragePooling()\n",
    "# Define a transformation to preprocess the input image\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Lambda(lambda x: x[:3, :, :]),  # Remove alpha channel if present\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "# Folder containing images\n",
    "image_folder = 'Single_Spectogram_01'\n",
    "\n",
    "# List to store individual outputs\n",
    "All_audio_outputs = []\n",
    "\n",
    "# Process each image in the folder\n",
    "for filename in os.listdir(image_folder):\n",
    "    if filename.endswith('.png'):\n",
    "        image_path = os.path.join(image_folder, filename)\n",
    "        \n",
    "        # Load and preprocess the image\n",
    "        image = Image.open(image_path)\n",
    "        input_tensor = transform(image)\n",
    "        input_batch = input_tensor.unsqueeze(0)  # Add batch dimension\n",
    "\n",
    "        # Perform inference\n",
    "        with torch.no_grad():\n",
    "            # print(input_batch.shape)\n",
    "            audio_output = Resnet18(input_batch)\n",
    "            audio_output = pooling_layer(audio_output)\n",
    "            \n",
    "        \n",
    "        # Append the output to the list\n",
    "        All_audio_outputs.append(audio_output)\n",
    "\n",
    "All_audio_outputs = torch.stack(All_audio_outputs)\n",
    "All_audio_outputs = All_audio_outputs.squeeze(1)\n",
    "# print(All_audio_outputs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([6, 1, 512])\n"
     ]
    }
   ],
   "source": [
    "class Audio_TemporalMultiHeadAttention(nn.Module):\n",
    "    def __init__(self, m: int, H: int) -> None:\n",
    "        super().__init__()\n",
    "        self.H = H\n",
    "        self.m = m\n",
    "\n",
    "        #making sure s is divisible by  H, otherwise problem!\n",
    "        assert m%H == 0, \"dimensions of model are divisble by number of heads\"\n",
    "\n",
    "        self.dim_head = m // H\n",
    "        self.w_q = nn.Linear(m, m, bias = False)\n",
    "        self.w_k = nn.Linear(m, m, bias = False)\n",
    "        self.w_v = nn.Linear(m, m, bias = False)\n",
    "        self.w_o = nn.Linear(m, m, bias = False)\n",
    "\n",
    "    @staticmethod\n",
    "    def attention(query, key, value):\n",
    "        dim_head = query.shape[-1]\n",
    "        # Just apply the formula from the paper\n",
    "        # (batch, h, seq_len, d_k) --> (batch, h, seq_len, seq_len)\n",
    "        attention_scores = (query @ key.transpose(-2, -1)) / math.sqrt(dim_head)\n",
    "        attention_scores = attention_scores.softmax(dim=-1) # (batch, h, seq_len, seq_len) # Apply softmax\n",
    "        \n",
    "        # (batch, h, seq_len, seq_len) --> (batch, h, seq_len, d_k)\n",
    "        # return attention scores which can be used for visualization\n",
    "        return (attention_scores @ value), attention_scores\n",
    "\n",
    "    def forward(self, q):\n",
    "        query = self.w_q(q) # (batch, seq_len, d_model) --> (batch, seq_len, d_model)\n",
    "        key = self.w_k(q) # (batch, seq_len, d_model) --> (batch, seq_len, d_model)\n",
    "        value = self.w_v(q) # (batch, seq_len, d_model) --> (batch, seq_len, d_model)\n",
    "\n",
    "        # (batch, seq_len, d_model) --> (batch, seq_len, h, d_k) --> (batch, h, seq_len, d_k)\n",
    "        query = query.view(query.shape[0], query.shape[1], self.H, self.dim_head).transpose(1, 2)\n",
    "        key = key.view(key.shape[0], key.shape[1], self.H, self.dim_head).transpose(1, 2)\n",
    "        value = value.view(value.shape[0], value.shape[1], self.H, self.dim_head).transpose(1, 2)\n",
    "\n",
    "        # Calculate attention\n",
    "        x, self.attention_scores = Audio_TemporalMultiHeadAttention.attention(query, key, value)\n",
    "        \n",
    "        # Combine all the heads together\n",
    "        # (batch, h, seq_len, d_k) --> (batch, seq_len, h, d_k) --> (batch, seq_len, d_model)\n",
    "        x = x.transpose(1, 2).contiguous().view(x.shape[0], -1, self.H * self.dim_head)\n",
    "\n",
    "        # Multiply by Wo\n",
    "        # (batch, seq_len, d_model) --> (batch, seq_len, d_model)  \n",
    "        return self.w_o(x)\n",
    "        \n",
    "        \n",
    "\n",
    "Audio_selfAttention = Audio_TemporalMultiHeadAttention(512, 8)\n",
    "All_audio_outputs = All_audio_outputs.unsqueeze(1)\n",
    "output = Audio_selfAttention(All_audio_outputs)\n",
    "print(output.shape)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
