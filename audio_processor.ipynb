{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 1: Extract Audio Files from Video Clips in another folder: Audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Writing audio in Single_Audio_01\\01-02-01-01-01-01-01.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "from moviepy.editor import VideoFileClip\n",
    "import os\n",
    "\n",
    "def extract_audio_from_folder(input_folder, output_folder):\n",
    "    # Create the output folder if it doesn't exist\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "    # Process each video file in the input folder\n",
    "    for video_file in os.listdir(input_folder):\n",
    "        if video_file.endswith('.mp4'):\n",
    "            input_video_path = os.path.join(input_folder, video_file)\n",
    "            output_audio_path = os.path.join(output_folder, f'{os.path.splitext(video_file)[0]}.wav')\n",
    "\n",
    "            extract_audio(input_video_path, output_audio_path)\n",
    "\n",
    "def extract_audio(input_video_path, output_audio_path):\n",
    "    video_clip = VideoFileClip(input_video_path)\n",
    "    audio_clip = video_clip.audio\n",
    "\n",
    "    audio_clip.write_audiofile(output_audio_path, codec='pcm_s16le', fps=audio_clip.fps)\n",
    "\n",
    "    video_clip.close()\n",
    "\n",
    "# Example usage\n",
    "input_video_folder = 'Single_Actor_01'\n",
    "output_audio_folder = 'Single_Audio_01'\n",
    "\n",
    "extract_audio_from_folder(input_video_folder, output_audio_folder)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 2: Generate Normalized Spectograms from Audio Files present in folder: Audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "import librosa.display\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "def generate_spectrogram(audio_file, fft_size=256, hop_size=10, window_size=32, num_parts=6):\n",
    "    # Load audio file\n",
    "    y, sr = librosa.load(audio_file, sr=None, duration=4)\n",
    "\n",
    "    # Calculate the required padding for the spectrogram\n",
    "    n_fft = fft_size\n",
    "    hop_length = int(sr * hop_size / 1000)  # Convert hop_size from ms to samples\n",
    "    win_length = int(sr * window_size / 1000)  # Convert window_size from ms to samples\n",
    "\n",
    "    # Adjust the n_fft to be at least the length of the signal\n",
    "    n_fft = max(n_fft, len(y))\n",
    "\n",
    "    # Compute spectrogram\n",
    "    spectrogram = librosa.feature.melspectrogram(\n",
    "        y=y,\n",
    "        sr=sr,\n",
    "        n_fft=n_fft,\n",
    "        hop_length=hop_length,\n",
    "        win_length=win_length,\n",
    "        window=\"hamming\",\n",
    "        n_mels=256  # Number of frequency components\n",
    "    )\n",
    "\n",
    "    # Convert to decibels\n",
    "    spectrogram_db = librosa.power_to_db(spectrogram, ref=np.max)\n",
    "\n",
    "    # Split spectrogram into N shorter parts\n",
    "    part_size = spectrogram_db.shape[1] // num_parts\n",
    "    spectrogram_parts = [spectrogram_db[:, i * part_size:(i + 1) * part_size] for i in range(num_parts)]\n",
    "\n",
    "    return spectrogram_parts\n",
    "\n",
    "def normalize_sequences(sequences):\n",
    "    # Flatten the sequences to compute mean and variance\n",
    "    flat_sequences = np.concatenate(sequences, axis=1)\n",
    "\n",
    "    # Compute mean and variance\n",
    "    mean = np.mean(flat_sequences, axis=1, keepdims=True)\n",
    "    std = np.std(flat_sequences, axis=1, keepdims=True)\n",
    "\n",
    "    # Normalize sequences\n",
    "    normalized_sequences = [(seq - mean) / std for seq in sequences]\n",
    "\n",
    "    return normalized_sequences\n",
    "    \n",
    "def save_normalized_spectrogram_images(audio_file_path, audio_file,normalized_parts, output_folder):\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "    y, sr = librosa.load(audio_file_path, sr=None, duration=4)\n",
    "    hop_length = int(sr * 10 / 1000)\n",
    "    \n",
    "    for i, part in enumerate(normalized_parts):\n",
    "        # Plot the normalized spectrogram without labels\n",
    "        plt.figure(figsize=(6, 4))\n",
    "        librosa.display.specshow(part, sr=sr, hop_length=hop_length, x_axis=None, y_axis=None)\n",
    "        plt.axis('off')\n",
    "\n",
    "        # Save the image\n",
    "        image_path = os.path.join(output_folder, f'{os.path.splitext(audio_file)[0]}-0{i+1}.png')\n",
    "        plt.savefig(image_path, bbox_inches='tight', pad_inches=0)\n",
    "        plt.close()\n",
    "\n",
    "def save_normalized_spectrogram_images_from_folder(input_folder, output_folder):\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "    # Process each audio file in the input folder\n",
    "    for audio_file in os.listdir(input_folder):\n",
    "        if audio_file.endswith('.wav'):\n",
    "            audio_file_path = os.path.join(input_folder, audio_file)\n",
    "            spectrogram_parts = generate_spectrogram(audio_file_path)\n",
    "            normalized_parts = normalize_sequences(spectrogram_parts)\n",
    "            save_normalized_spectrogram_images(audio_file_path, audio_file, normalized_parts, output_folder)\n",
    "            \n",
    "\n",
    "# Example usage\n",
    "input_audio_folder = 'Single_Audio_01'\n",
    "output_spectrogram_folder = 'Single_Spectogram_01'\n",
    "\n",
    "save_normalized_spectrogram_images_from_folder(input_audio_folder, output_spectrogram_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 3: Get the output from resnet-18 and also apply spatial pooling afterwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\User/.cache\\torch\\hub\\pytorch_vision_v0.10.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 512, 1, 1])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "import torch.nn as nn\n",
    "import os\n",
    "import warnings\n",
    "\n",
    "# Suppress all warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "resnet18 = torch.hub.load('pytorch/vision:v0.10.0', 'resnet18', pretrained=True)\n",
    "warnings.resetwarnings()\n",
    "# Remove global average pooling layer\n",
    "resnet18 = nn.Sequential(*list(resnet18.children())[:-1])\n",
    "\n",
    "# Add spatial average pooling layer\n",
    "resnet18.add_module('avgpool', nn.AdaptiveAvgPool2d(1))\n",
    "\n",
    "resnet18.eval()\n",
    "\n",
    "\n",
    "# Define a transformation to preprocess the input image\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "     transforms.Lambda(lambda x: x[:3, :, :]),  # Remove alpha channel if present\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "# Folder containing images\n",
    "image_folder = 'Spectogram_01'\n",
    "\n",
    "# List to store individual outputs\n",
    "All_audio_outputs = []\n",
    "\n",
    "# Process each image in the folder\n",
    "for filename in os.listdir(image_folder):\n",
    "    if filename.endswith('.png'):\n",
    "        image_path = os.path.join(image_folder, filename)\n",
    "        \n",
    "        # Load and preprocess the image\n",
    "        image = Image.open(image_path)\n",
    "        input_tensor = transform(image)\n",
    "        input_batch = input_tensor.unsqueeze(0)  # Add batch dimension\n",
    "\n",
    "        # Perform inference\n",
    "        with torch.no_grad():\n",
    "            audio_output = resnet18(input_batch)\n",
    "        \n",
    "        # Append the output to the list\n",
    "        All_audio_outputs.append(audio_output)\n",
    "\n",
    "print(All_audio_outputs[0].shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
