{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Visual_SpatialMultiHeadAttention(nn.Module):\n",
    "    def __init__(self, m: int, H: int) -> None:\n",
    "        super().__init__()\n",
    "        self.m = m\n",
    "        self.H = H\n",
    "        \n",
    "        #making sure m is divisible by  H, otherwise problem!\n",
    "        assert m % H == 0, \"dimensions of model are divisible by number of heads\"\n",
    "        \n",
    "        self.dim_head = m // H\n",
    "        self.w_q = nn.Linear(m, m, bias = False)\n",
    "        self.w_k = nn.Linear(m, m, bias = False)\n",
    "        self.w_v = nn.Linear(m, m, bias = False)\n",
    "        self.w_o = nn.Linear(m, m, bias = False)\n",
    "        \n",
    "    \n",
    "    @staticmethod\n",
    "    def attention(query, key, value):\n",
    "        dim_head = query.shape[-1]\n",
    "        # Just apply the formula from the paper\n",
    "        # (batch, h, seq_len, d_k) --> (batch, h, seq_len, seq_len)\n",
    "        attention_scores = (query @ key.transpose(-2, -1)) / math.sqrt(dim_head)\n",
    "        attention_scores = attention_scores.softmax(dim=-1) # (batch, h, seq_len, seq_len) # Apply softmax\n",
    "        \n",
    "        # (batch, h, seq_len, seq_len) --> (batch, h, seq_len, d_k)\n",
    "        # return attention scores which can be used for visualization\n",
    "        return (attention_scores @ value), attention_scores\n",
    "\n",
    "    def forward(self, q):\n",
    "        query = self.w_q(q) # (batch, seq_len, d_model) --> (batch, seq_len, d_model)\n",
    "        key = self.w_k(q) # (batch, seq_len, d_model) --> (batch, seq_len, d_model)\n",
    "        value = self.w_v(q) # (batch, seq_len, d_model) --> (batch, seq_len, d_model)\n",
    "\n",
    "        # (batch, seq_len, d_model) --> (batch, seq_len, h, d_k) --> (batch, h, seq_len, d_k)\n",
    "        query = query.view(query.shape[0], query.shape[1], self.H, self.dim_head).transpose(1, 2)\n",
    "        key = key.view(key.shape[0], key.shape[1], self.H, self.dim_head).transpose(1, 2)\n",
    "        value = value.view(value.shape[0], value.shape[1], self.H, self.dim_head).transpose(1, 2)\n",
    "\n",
    "        # Calculate attention\n",
    "        x, self.attention_scores = Visual_SpatialMultiHeadAttention.attention(query, key, value)\n",
    "        \n",
    "        # Combine all the heads together\n",
    "        # (batch, h, seq_len, d_k) --> (batch, seq_len, h, d_k) --> (batch, seq_len, d_model)\n",
    "        x = x.transpose(1, 2).contiguous().view(x.shape[0], -1, self.H * self.dim_head)\n",
    "\n",
    "        # Multiply by Wo\n",
    "        # (batch, seq_len, d_model) --> (batch, seq_len, d_model)  \n",
    "        return self.w_o(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Visual_ChannelMultiHeadAttention(nn.Module):\n",
    "    def __init__(self, s: int, H: int) -> None:\n",
    "        super().__init__()\n",
    "        self.s = s\n",
    "        self.H = H\n",
    "\n",
    "        #making sure s is divisible by  H, otherwise problem!\n",
    "        assert s%H == 0, \"dimensions of model are divisble by number of heads\"\n",
    "\n",
    "        self.dim_head = s // H\n",
    "        self.w_q = nn.Linear(s, s, bias = False)\n",
    "        self.w_k = nn.Linear(s, s, bias = False)\n",
    "        self.w_v = nn.Linear(s, s, bias = False)\n",
    "        self.w_o = nn.Linear(s, s, bias = False)\n",
    "\n",
    "    @staticmethod\n",
    "    def attention(query, key, value):\n",
    "        dim_head = query.shape[-1]\n",
    "        # Just apply the formula from the paper\n",
    "        # (batch, h, seq_len, d_k) --> (batch, h, seq_len, seq_len)\n",
    "        attention_scores = (query @ key.transpose(-2, -1)) / math.sqrt(dim_head)\n",
    "        attention_scores = attention_scores.softmax(dim=-1) # (batch, h, seq_len, seq_len) # Apply softmax\n",
    "        \n",
    "        # (batch, h, seq_len, seq_len) --> (batch, h, seq_len, d_k)\n",
    "        # return attention scores which can be used for visualization\n",
    "        return (attention_scores @ value), attention_scores\n",
    "\n",
    "    def forward(self, q):\n",
    "        query = self.w_q(q) # (batch, seq_len, d_model) --> (batch, seq_len, d_model)\n",
    "        key = self.w_k(q) # (batch, seq_len, d_model) --> (batch, seq_len, d_model)\n",
    "        value = self.w_v(q) # (batch, seq_len, d_model) --> (batch, seq_len, d_model)\n",
    "\n",
    "        # (batch, seq_len, d_model) --> (batch, seq_len, h, d_k) --> (batch, h, seq_len, d_k)\n",
    "        query = query.view(query.shape[0], query.shape[1], self.H, self.dim_head).transpose(1, 2)\n",
    "        key = key.view(key.shape[0], key.shape[1], self.H, self.dim_head).transpose(1, 2)\n",
    "        value = value.view(value.shape[0], value.shape[1], self.H, self.dim_head).transpose(1, 2)\n",
    "\n",
    "        # Calculate attention\n",
    "        x, self.attention_scores = Visual_ChannelMultiHeadAttention.attention(query, key, value)\n",
    "        \n",
    "        # Combine all the heads together\n",
    "        # (batch, h, seq_len, d_k) --> (batch, seq_len, h, d_k) --> (batch, seq_len, d_model)\n",
    "        x = x.transpose(1, 2).contiguous().view(x.shape[0], -1, self.H * self.dim_head)\n",
    "\n",
    "        # Multiply by Wo\n",
    "        # (batch, seq_len, d_model) --> (batch, seq_len, d_model)  \n",
    "        return self.w_o(x)\n",
    "        \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Visual_TemporalMultiHeadAttention(nn.Module):\n",
    "    def __init__(self, m: int, H: int) -> None:\n",
    "        super().__init__()\n",
    "        self.H = H\n",
    "        self.m = m\n",
    "\n",
    "        #making sure s is divisible by  H, otherwise problem!\n",
    "        assert m%H == 0, \"dimensions of model are divisble by number of heads\"\n",
    "\n",
    "        self.dim_head = m // H\n",
    "        self.w_q = nn.Linear(m, m, bias = False)\n",
    "        self.w_k = nn.Linear(m, m, bias = False)\n",
    "        self.w_v = nn.Linear(m, m, bias = False)\n",
    "        self.w_o = nn.Linear(m, self.dim_head, bias = False)\n",
    "\n",
    "    @staticmethod\n",
    "    def attention(query, key, value):\n",
    "        dim_head = query.shape[-1]\n",
    "        # Just apply the formula from the paper\n",
    "        # (batch, h, seq_len, d_k) --> (batch, h, seq_len, seq_len)\n",
    "        attention_scores = (query @ key.transpose(-2, -1)) / math.sqrt(dim_head)\n",
    "        attention_scores = attention_scores.softmax(dim=-1) # (batch, h, seq_len, seq_len) # Apply softmax\n",
    "        \n",
    "        # (batch, h, seq_len, seq_len) --> (batch, h, seq_len, d_k)\n",
    "        # return attention scores which can be used for visualization\n",
    "        return (attention_scores @ value), attention_scores\n",
    "\n",
    "    def forward(self, q):\n",
    "        query = self.w_q(q) # (batch, seq_len, d_model) --> (batch, seq_len, d_model)\n",
    "        key = self.w_k(q) # (batch, seq_len, d_model) --> (batch, seq_len, d_model)\n",
    "        value = self.w_v(q) # (batch, seq_len, d_model) --> (batch, seq_len, d_model)\n",
    "\n",
    "        # (batch, seq_len, d_model) --> (batch, seq_len, h, d_k) --> (batch, h, seq_len, d_k)\n",
    "        query = query.view(query.shape[0], query.shape[1], self.H, self.dim_head).transpose(1, 2)\n",
    "        key = key.view(key.shape[0], key.shape[1], self.H, self.dim_head).transpose(1, 2)\n",
    "        value = value.view(value.shape[0], value.shape[1], self.H, self.dim_head).transpose(1, 2)\n",
    "\n",
    "        # Calculate attention\n",
    "        x, self.attention_scores = Visual_TemporalMultiHeadAttention.attention(query, key, value)\n",
    "        \n",
    "        # Combine all the heads together\n",
    "        # (batch, h, seq_len, d_k) --> (batch, seq_len, h, d_k) --> (batch, seq_len, d_model)\n",
    "        x = x.transpose(1, 2).contiguous().view(x.shape[0], -1, self.H * self.dim_head)\n",
    "\n",
    "        # Multiply by Wo\n",
    "        # (batch, seq_len, d_model) --> (batch, seq_len, d_model)  \n",
    "        return self.w_o(x)\n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpatialAveragePooling(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SpatialAveragePooling, self).__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        #dimension of x --> N x m x s\n",
    "        pooled = torch.mean(x, dim=2)\n",
    "        return pooled\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Visual_BeforeCross(nn.Module):\n",
    "    def __init__(self, spatial: Visual_SpatialMultiHeadAttention, channel: Visual_ChannelMultiHeadAttention, sap: SpatialAveragePooling, temporal:Visual_TemporalMultiHeadAttention) -> None:\n",
    "        super().__init__()\n",
    "        self.spatial_selfAttention = spatial\n",
    "        self.channel_selfAttention = channel\n",
    "        self.pool = sap\n",
    "        self.temporal_selfAttention = temporal\n",
    "\n",
    "        #making sure s is divisible by  H, otherwise problem!\n",
    "        # assert m%H == 0, \"dimensions of model are divisble by number of heads\"\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.spatial_selfAttention(x)\n",
    "        print(\"After Spatial\")\n",
    "        print(x.shape)\n",
    "        x = x.transpose(1,2)\n",
    "        x = self.channel_selfAttention(x)\n",
    "        print(\"After Channel\")\n",
    "        print(x.shape)\n",
    "        x = self.pool(x)\n",
    "        print(\"After pool\")\n",
    "        print(x.shape)\n",
    "        x = x.unsqueeze(1)\n",
    "        x = self.temporal_selfAttention(x)\n",
    "        print(\"After Temporal\")\n",
    "        x= x.squeeze(1)\n",
    "        print(x.shape)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After Spatial\n",
      "torch.Size([6, 64, 2048])\n",
      "After Channel\n",
      "torch.Size([6, 2048, 64])\n",
      "After pool\n",
      "torch.Size([6, 2048])\n",
      "After Temporal\n",
      "torch.Size([6, 256])\n"
     ]
    }
   ],
   "source": [
    "m = 2048\n",
    "s = 64\n",
    "H = 8\n",
    "\n",
    "input_tensor = torch.randn(6, s, m)\n",
    "\n",
    "spatialSA = Visual_SpatialMultiHeadAttention(m, H)\n",
    "\n",
    "channelSA = Visual_ChannelMultiHeadAttention(s, H)\n",
    "\n",
    "sap = SpatialAveragePooling() \n",
    "\n",
    "temporalSA = Visual_TemporalMultiHeadAttention(m, H)\n",
    "\n",
    "v = Visual_BeforeCross(spatialSA, channelSA, sap, temporalSA)\n",
    "output_tensor = v.forward(input_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class Audio_TemporalMultiHeadAttention(nn.Module):\n",
    "#     def __init__(self, m: int, H: int) -> None:\n",
    "#         super().__init__()\n",
    "#         self.H = H\n",
    "#         self.m = m\n",
    "\n",
    "#         #making sure s is divisible by  H, otherwise problem!\n",
    "#         assert m%H == 0, \"dimensions of model are divisble by number of heads\"\n",
    "\n",
    "#         self.dim_head = m // H\n",
    "#         self.w_q = nn.Linear(m, m, bias = False)\n",
    "#         self.w_k = nn.Linear(m, m, bias = False)\n",
    "#         self.w_v = nn.Linear(m, m, bias = False)\n",
    "#         self.w_o = nn.Linear(m, m, bias = False)\n",
    "\n",
    "#     @staticmethod\n",
    "#     def attention(query, key, value):\n",
    "#         dim_head = query.shape[-1]\n",
    "#         # Just apply the formula from the paper\n",
    "#         # (batch, h, seq_len, d_k) --> (batch, h, seq_len, seq_len)\n",
    "#         attention_scores = (query @ key.transpose(-2, -1)) / math.sqrt(dim_head)\n",
    "#         attention_scores = attention_scores.softmax(dim=-1) # (batch, h, seq_len, seq_len) # Apply softmax\n",
    "        \n",
    "#         # (batch, h, seq_len, seq_len) --> (batch, h, seq_len, d_k)\n",
    "#         # return attention scores which can be used for visualization\n",
    "#         return (attention_scores @ value), attention_scores\n",
    "\n",
    "#     def forward(self, q, k, v):\n",
    "#         query = self.w_q(q) # (batch, seq_len, d_model) --> (batch, seq_len, d_model)\n",
    "#         key = self.w_k(k) # (batch, seq_len, d_model) --> (batch, seq_len, d_model)\n",
    "#         value = self.w_v(v) # (batch, seq_len, d_model) --> (batch, seq_len, d_model)\n",
    "\n",
    "#         # (batch, seq_len, d_model) --> (batch, seq_len, h, d_k) --> (batch, h, seq_len, d_k)\n",
    "#         query = query.view(query.shape[0], query.shape[1], self.H, self.dim_head).transpose(1, 2)\n",
    "#         key = key.view(key.shape[0], key.shape[1], self.H, self.dim_head).transpose(1, 2)\n",
    "#         value = value.view(value.shape[0], value.shape[1], self.H, self.dim_head).transpose(1, 2)\n",
    "\n",
    "#         # Calculate attention\n",
    "#         x, self.attention_scores = Visual_SpatialMultiHeadAttention.attention(query, key, value)\n",
    "        \n",
    "#         # Combine all the heads together\n",
    "#         # (batch, h, seq_len, d_k) --> (batch, seq_len, h, d_k) --> (batch, seq_len, d_model)\n",
    "#         x = x.transpose(1, 2).contiguous().view(x.shape[0], -1, self.H * self.dim_head)\n",
    "\n",
    "#         # Multiply by Wo\n",
    "#         # (batch, seq_len, d_model) --> (batch, seq_len, d_model)  \n",
    "#         return self.w_o(x)\n",
    "        \n",
    "        \n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
